# Security and Privacy of Machine Learning, Spring 2024, Assignment 1

This repository provides scripts for generating adversarial examples using the Carlini-Wagner (CW) attack and evaluating the performance of models on both adversarial and original datasets.

## Adversarial Example Generation

1. Create a directory named `cifar-100_CW_mix`.
2. Within the `cifar-100_CW_mix` directory, create another directory named `images`.
3. Run the following command to generate adversarial examples based on the CW attack:

```python
python cw_attack.py
```
The generated adversarial examples will be stored in the `cifar-100_CW_mix/images` folder.

## Evaluation

1. Create a directory named `cifar-100_eval`.
2. Within the `cifar-100_eval` directory, create another directory named `images`.
3. Place all 500 CIFAR-100 evaluation images into the `images` directory.
4. To start the evaluation process, run the following command:

```python
python main.py -m <mode>
```

Replace `<mode>` with either `cw` or `eval`:

- `cw`: This mode will test all the models against the dataset generated by the CW attack.
- `eval`: This mode will test the models on the original evaluation set.


